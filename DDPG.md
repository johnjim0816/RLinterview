## 什么是硬更新和软更新？

软更新(soft update)的概念应该首先出现自DDPG论文中，与硬更新相对应。硬更新在DQN中就出现过，DQN中为了实现稳定的值估计而引入了一个目标网络(tatget net，通常用$\hat{Q}$表示)。与行为网络(policy net，通常用$Q$表示)每步更新参数不同，目标网络会以一个较慢的频率，比如每隔$C$步复制行为网络的参数，即$\hat{\theta} \leftarrow \theta$。而DDPG为了进一步解决稳定的值估计问题，提出了软更新，即$\hat{\theta} \leftarrow \tau\theta + (1-\tau)\hat{\theta}$，$\tau$是一个处于0到1之间的超参数，其实我们在深度学习也有类似的技巧，例如梯度下降的momentum优化器。当然不光在DDPG中可以运用软更新，在所有涉及目标网络的RL算法实践中在有必要的情况下可以使用软更新，包括DQN、C51算法等等。